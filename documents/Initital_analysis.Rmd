---
title: "Initial analysis - dissertations"
author: "Aleksandra Wilczynska"
date: "28-2-2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F)

# Load libraries ----------------------------------------------------------

library(tidyverse)
library(here)
library(data.table)
library(igraph)
library(ggraph)
library(stringr)
library(tm)
library(tidytext)

# Load the data table  - each row - different publication  ------------------------------------------------
text_data<-readRDS(here('data_output', 'text_data.RDS'))

```

## Introduction

This is a first attempt to analyse the dissertation texts from IDE faculty. The analysis is made using the following resources:

- [Tidy text mining](https://www.tidytextmining.com/)
- [Text mining in R - video](https://www.youtube.com/watch?v=ae_XVhjHd_o)
- [Basisc](https://sicss.io/2018/materials/day3-text-analysis/basic-text-analysis/rmarkdown/Basic_Text_Analysis_in_R.html)

## Preparing the dataset

### Extracting tokens

The first step in the analysis: create a dataset with tokens (in this case words) as units of analysis. One row per token.

```{r words_table}
# Divide thesis text into words into words ---------------------------------
text_by_words <- unnest_tokens(text_data, word, full_text_lower) # divide by words
text_by_words <- text_by_words[!as.data.table(stop_words), on = "word"] # remove common stop words
text_by_words <- text_by_words[str_detect(word, '[a-zA-Z]')] # keep only strings which include at least one letter

# Create data.table with separate words only -------------------------------
#names(text_by_words)
words <- text_by_words[,.N, by = .(filename, word)] # data.table with word count by publication



# Create a dataset with the publications in which the words have not appeared
filenames <- setDT(words[, expand.grid(unique(filename),unique(word))  ])
setnames(filenames,c('filename', 'word') )

setkey(words, filename,word)
setkey(filenames, filename,word)
words_exploded <- words[filenames]
setnafill(words_exploded, cols="N", fill=0)


```

### Summary statistics 

Add auxiliary statistics: 

- Count of words across all publications
- Count of publications the word appeared in
- Percentage a word constitutes of all words within a publication
- Percentage of publications where the words is mentioned

```{r words_stats}
# Words with count of publications they showed up in 
words_exploded[,`:=`(N_publication = sum(N>0), # count of publications they showed up in  
            N_total = sum(N) # count of times the words showed up in total
), by = word]#[order(-N_total)]

words[,`:=`(N_publication = sum(N>0), # count of publications they showed up in  
            N_total = sum(N) # count of times the words showed up in total
), by = word]#[order(-N_total)]


words_exploded[, N_words_publ := sum(N), by =filename]# count number of words in dissertation


# Relative terms 
words_exploded[,`:=`(pct_per_publ = round((N/N_words_publ)*100), # pct of all words within publication
            pct_publ = round((N_publication/length(unique(filename)))*100) # pct of publications 
            )]

# see the distribution of words, find a cutoff 
boxplot(words_exploded$N_total) 


x<-words_exploded[order(-N_total),.(word,N_publication, N_total, pct_publ)] %>%
  unique() %>%
  head(10)%>%
  knitr::kable(caption = "Top 10 words") 
  
```

### Additional cleaning 

The popular stopwords have been removed in earlier stage, but some additional tweaks can be performed to clean the dataset of not meaningful words.

Some considerations: 

- Popular words concentrated in one publication 
  - Potentially center of one topic, rather than trend (e.g. 'toilet') - cross-reference with title when available in the metadata
- Popular words evenly spread throughout all/many publications 
  - Potentially the trend/ important words

- Popular words evenly spread throughout all/many publications but not conveying any meaning 
  - Additional stop words 
  - Might look for those with non-letters 
  - Is there a way to automate the cleaning - or semi-automate?
- Words appearing in all publications bu only once/ a few times 
  - Potentially coming from metadata/title page
- Additional cleaning: 
  - Remove word 'variations' e.g. user vs users; design vs designing etc.? (parallelisation?)

```{r popular_concentrated}
# Additional cleaning -----------------------------------------------------------


# Popular words concentrated in one (few) publication
## Potentially center of one topic, rather than trend (e.g. 'toilet') - cross-reference with title when available in the metadata

# Add a measure of dispersion ( here: STD/ Mean, should we include some other?)

# Average N of words in a publication and std 
words_exploded[, `:=`(Avg_N_publication = mean(N), # average words per publication
                      Std_N_publication = sd(N) # standard deviation of number of words per publication
                      ), by = word]

words_exploded[,Concentration := Std_N_publication/Avg_N_publication ] #metric of dispersion: if standard deviation higher than the mean, than can be considered as dispersed

hist(words_exploded[N_publication>1,Concentration], main = 'Concentration of words appearing in more than one publication')

words_exploded[N_total>1000 & Concentration>=2.5 , .(word, N_total, N_publication,
                                                  Concentration)][order(-N_total)] %>% 
  unique() %>%
  knitr::kable(caption = "Words mentioned >1000 times (overall) and concentrated in one/few publications")# showing words concentrated in few works, including also those appearing only in one publication (Concentration measure not available for those)


```
 
Looking at the result above, anything with the `Dispersion` measure above 2.5 would be considered as high concentration - these words would naturally affect more the overall result of the analysis with the higher number of dissertations at hand. With 16, they might be highly influential. This can be offset by controlling for the metadata ( title/ keywords).      


```{r  popular_evenly}
# Popular words evenly spread throughout all/many publications 

words_exploded[N_total>1000 & Concentration<1.5 , .(word, N_total, N_publication,
                                                  Concentration)][order(-N_total)]%>% 
  unique() %>%
  knitr::kable(caption = "Popular words evenly spread throughout all (many) publications ")

```


```{r stopwords_nonalhpa}

# Popular words evenly spread throughout all/many publications but not conveying any meaning - additional stop words - might look for those with non-letters ( is there a way to automate the cleaning - or semi-automate?)

words[stringr::str_detect(word,'[^a-zA-Z]')][order(-N_total)][N_total>100,.(word,N_total)] %>%
  unique() %>% 
  knitr::kable(caption ='Popular words including non-letter')

stop_words_add <- c('doi.org', 'e.g', 'i.e')

```

Some of the words with non-letters can be cut ('doi.org', 'e.g' , 'i.e'  ). Manual check still needed , as there are are words that are very much essential ('3d'). 


```{r stopwords_short }
# Popular short words
words_exploded[N_total>100 & Concentration<1 & nchar(word)<5 , .(word, N_total, N_publication,
                                                  Concentration)][order(-N_total)] %>%
  unique() %>%
  head(20) %>%
  knitr::kable(caption = 'Popular short words ')

stop_words_add <- c(stop_words_add  , c('al', 'de', 'van', 'een', 'en', 'het','http', 'html', 'https') )

```
Some short words can be dismissed (al, et, van)


```{r evenly_sparse}
# Words appearing in all publications bu only once/ a few times  - potentially coming from metadata/title page)


```


```{r word_variations }
# remove word 'variations' e.g. user vs users; design vs designing etc.

# Finding similarities between the words / naive approach
# word_list <- unique(words$word)
# x<-word_list[1:100]
# y <- adist(x, x)
# colnames(y) <- x ; rownames(y) <- x
# y[lower.tri(y)] <- NA
```


```{r additional_cleaning}

# Remove highly concentrated popular words 
#words <- words[!word %in% words_exploded[N_total>1000 & Concentration > 3, word] ]

# Remove additional identified stopwords 
words <- words[!word %in% stop_words_add]



```


## Visualisation



## Topic modelling 

Packages : 
 - pdftools
 - tabulizer 
 - tidytext
 - pluralize ( singular out of plurals)
 - gc()
 -bi_gram()
- library(quanteda)
- library(readtext)
- library(wordnet)
- library(SnowballC)
 
 

